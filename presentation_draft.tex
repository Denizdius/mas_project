
\documentclass{beamer}
\usetheme{Madrid}
\usecolortheme{whale}

\title{Automated Related Work Generation with Multi-Agent Systems}

\author{Hatice Nur Kömürcü \& Deniz Karabacak}
\date{\today}

\begin{document}

% -----------------------------------------------------------------------------
% SLIDE 1: Title
% -----------------------------------------------------------------------------
\frame{\titlepage}

% -----------------------------------------------------------------------------
% SLIDE 2: Structure
% -----------------------------------------------------------------------------
\begin{frame}{Presentation Outline}
    \tableofcontents
\end{frame}

% -----------------------------------------------------------------------------
% SECTION 1: INTRODUCTION
% -----------------------------------------------------------------------------
\section{Introduction}

\begin{frame}{Problem Statement}
    \begin{itemize}
        \item \textbf{The Challenge:} Writing a "Related Work" section requires synthesizing dozens of papers, identifying gaps, and maintaining strict citation accuracy.
        \item \textbf{Current Limitations:} 
        \begin{itemize}
            \item Humans: Time-consuming and prone to missing recent papers.
            \item Single LLMs: prone to \textbf{hallucinations} (inventing citations), limited context windows, and superficial summarization rather than synthesis.
        \end{itemize}
        \item \textbf{Goal:} Automate the generation of publication-quality Related Work sections that are factually grounded and thematically organized.
    \end{itemize}
\end{frame}

\begin{frame}{Need for Multi-Agent Methodology}
    Why not just use a single agent?
    \begin{itemize}
        \item \textbf{Cognitive Load:} A single agent struggles to hold 10-20 papers in context while simultaneously planning structure, checking citations, and writing a draft.
        \item \textbf{Separation of Concerns:}
        \begin{itemize}
            \item \textit{Planning} requires high-level reasoning.
            \item \textit{Writing} requires creative construction.
            \item \textit{Critiquing} requires strict fact-checking.
        \end{itemize}
        \item \textbf{Parallelism:} MAS allow us to process different themes concurrently, improving scalability.
    \end{itemize}
\end{frame}



% -----------------------------------------------------------------------------
% SECTION 2: METHODOLOGY
% -----------------------------------------------------------------------------
\section{Methodology}

\begin{frame}{Single Agent Baseline}
    \textbf{Method Overview:}
    \begin{itemize}
        \item A linear approach.
        \item \textbf{Workflow:}
        \begin{enumerate}
            \item \textbf{Retrieve:} Fetches top 5 papers via ArXiv API.
            \item \textbf{Generate:} LLM receives all abstracts in one prompt and generates the full text.
            \item \textbf{Refine:} A simple self-correction loop checks for basic errors.
        \end{enumerate}
        \item \textbf{Limitation:} Often hallucinates citations or lists papers sequentially without reasoning.
    \end{itemize}
\end{frame}



\begin{frame}{Multi-Agent Environment (Model Overview)}
    A collaborative architecture with 4 specialized roles:
    \vspace{0.2cm}
    \begin{enumerate}
        \item \textbf{Planner Agent:} Analyzes all retrieved abstracts (10+) to extract key research themes.
        \item \textbf{Outliner Agent:} Structures the paper into logical sections based on themes.
        \item \textbf{Writer Agents (Parallel):} 
        \begin{itemize}
            \item Multiple instances run concurrently.
            \item Each writes one specific section using strict citation constraints.
        \end{itemize}
        \item \textbf{Critic Agent:} 
        \begin{itemize}
            \item Grades output on \textbf{Grounding, Coherence, Completeness}.
            \item Rejects drafts with missing citations (less than 8/10 score).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}{Multi-Agent System Architecture Diagram}
    \begin{center}
        \includegraphics[width=\textwidth, keepaspectratio]{multi_agent_workflow.png}
    \end{center}
\end{frame}

\begin{frame}{LLM Methodology \& Technical Stack}
    \begin{itemize}
        \item \textbf{Core Model:} Qwen3-8B (Quantized w4a16) running on vLLM.
        \item \textbf{Reasoning:} Enabled "Thinking Mode" for complex reasoning tasks.
        \item \textbf{Management:} 
        \begin{itemize}
            \item \textbf{LangGraph:} Manages state and cyclic workflow (Writer $\leftrightarrow$ Critic loop).
            \item \textbf{ThreadPoolExecutor:} Enables parallel execution of writer agents for speed.
        \end{itemize}
        \item \textbf{Data Handling:} ArXiv API for retrieval, Regex for citation matching.
    \end{itemize}
\end{frame}

% -----------------------------------------------------------------------------
% SECTION 3: EXPERIMENTAL SETUP
% -----------------------------------------------------------------------------
\section{Experimental Setup}

\begin{frame}{Dataset}
    \textbf{OARelatedWork (Open Access Related Work)}
    \begin{itemize}
        \item A specialized dataset of scientific papers containing:
        \begin{itemize}
            \item Target Abstract (The paper being written).
            \item Ground Truth References (The actual bibliography).
            \item Ground Truth Related Work Section.
        \end{itemize}
        \item \textbf{Filtering:} We selected "High-Reference Samples" (min 7 citations) to test the system's ability to handle complex loads.
    \end{itemize}
\end{frame}

\begin{frame}{Evaluation Metrics}
    We use a multi-dimensional evaluation strategy:
    \vspace{0.2cm}
    \begin{itemize}
        \item \textbf{Citation Quality (The "Scientific Truth" Metric):}
        \begin{itemize}
            \item \textit{Recall:} How many relevant papers did we cite?
            \item \textit{Precision:} Did we hallucinate any papers?
        \end{itemize}
        \item \textbf{Semantic Similarity:}
        \begin{itemize}
            \item \textit{LSA (Latent Semantic Analysis):} Measures topic-level overlap (Context-aware).
            \item \textit{BERTScore:} Neural embedding similarity.
        \end{itemize}
        \item \textbf{Lexical Overlap:} ROUGE-1, ROUGE-2, ROUGE-L.
        \item \textbf{GRC Scores (Human-Aligned):} Grounding, Coherence, Completeness (graded by a Critic LLM).
    \end{itemize}
\end{frame}

\begin{frame}{Hardware \& Resources}
    \textbf{Technical Infrastructure:}
    \begin{itemize}
        \item \textbf{GPU:} NVIDIA RTX 4080 Laptop GPU (12GB VRAM).
        \item \textbf{Inference Server:} vLLM for high-throughput serving.
        \item \textbf{Quantization:} 4-bit quantization (w4a16) to fit the Qwen3-8B model and large context windows (16k tokens) into memory.
    \end{itemize}
\end{frame}

% -----------------------------------------------------------------------------
% SECTION 4: RESULTS
% -----------------------------------------------------------------------------
\section{Results}

\begin{frame}{Comparison: Single vs. Multi-Agent}
    \begin{table}[]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{|l|c|c|c|}
    \hline
    \textbf{Metric} & \textbf{Single Agent} & \textbf{Multi-Agent (MAS)} & \textbf{Improvement} \\ \hline
    Citation Recall & 0.721 & \textbf{1.690} & \textbf{+134\%} \\ \hline
    Citation Precision & 0.796 & \textbf{0.960} & \textbf{+21\%} \\ \hline
    Grounding (GRC) & 4.7 / 10 & \textbf{8.15 / 10} & \textbf{+73\%} \\ \hline
    Completeness (GRC) & 6.1 / 10 & \textbf{7.3 / 10} & +20\% \\ \hline
    BERTScore & 0.840 & 0.836 & $\approx$ Tie \\ \hline
    LSA Similarity & \textbf{0.622} & 0.401 & -35\% (Synthesis) \\ \hline
    ROUGE-1 & \textbf{0.392} & 0.383 & -2\% \\ \hline
    \end{tabular}%
    }
    \caption{Performance comparison on High-Reference samples (n=10).}
    \end{table}
    
    \vspace{0.2cm}
    \textbf{Key Insight:} The Single Agent is faster but misses \textbf{28\%} of citations (Recall < 1.0) and hallucinates frequently (Grounding 4.7). The MAS is rigorous, finding \textbf{69\% more} citations than the ground truth baseline references on average, with near-perfect precision (96\%).
\end{frame}

\begin{frame}{Time Complexity}
    \begin{itemize}
        \item \textbf{Single Agent:} 67 seconds per sample. (Linear)
        \item \textbf{Multi-Agent:} 318 seconds per sample. (Iterative)
        \item \textbf{Trade-off:} 
        \begin{itemize}
            \item MAS is $\sim$4.7x slower but produces quality drafts.
            \item Parallel processing (Writer Agents) reduced MAS time by 40\% compared to sequential execution.
        \end{itemize}
    \end{itemize}
\end{frame}

% -----------------------------------------------------------------------------
% SECTION 5: CONCLUSION
% -----------------------------------------------------------------------------
\section{Conclusion}

\begin{frame}{Conclusion}
    \begin{itemize}
        \item We presented a  Multi-Agent System for automating academic literature reviews.
        \item By decomposing the task into Planning, Drafting, and Critiquing, we significantly outperformed single-agent baselines in \textbf{factual accuracy} and \textbf{citation coverage}.
        \item The introduction of a "Critic" loop ensures that hallucinations are caught before the final output.
        \item \textbf{Future Work:}
        \begin{itemize}
            \item Integration of a Citation Graph Database for better "Gap Analysis".
            \item Dynamic fine-tuning of the Critic Agent on human feedback.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \centering
    \Huge Thank You!
    
    \vspace{1cm}
    \large Questions?
\end{frame}

\end{document}
